{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package words to\n[nltk_data]     /Users/andybryant/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words())\n",
    "nltk_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_from_txt(filepath):\n",
    "    '''Helper function for opening, reading, and concerting .txt file of vectors. \n",
    "    Returns a dictionary of the embeddings vectors.'''\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embeddings_dict = {line[0]: np.array(list(map(float, line[1:]))) for line in reader}\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_keys(keys):\n",
    "    '''Helper function for filtering stopwords and non-english words from a list of keys.\n",
    "    Returns a list of strings.'''\n",
    "    words = []\n",
    "    for key in keys:\n",
    "        if key not in nltk_stopwords and key in nltk_words:\n",
    "            words.append(key)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_dict(old_dict, words_to_keep):\n",
    "    '''Helper function for filtering a dictionary, keeping a specific list of words.\n",
    "    Returns a dictionary with words as keys and vectors as values.'''\n",
    "    new_dict = dict()\n",
    "    for w in words_to_keep:\n",
    "        val = old_dict.get(w)\n",
    "        # If dictionary has the word as a key, store its value\n",
    "        if val is not None:\n",
    "            new_dict[w] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_txt(filepath):\n",
    "    '''Helper function for generating dataframe with words as the index and the dimensions as columns.\n",
    "    Returns that dataframe.'''\n",
    "    vec_dict = get_dict_from_txt(filepath)\n",
    "    vec_keys = list(vec_dict.keys())\n",
    "    vec_words = get_words_from_keys(vec_keys)\n",
    "    vec_dict_filtered = get_filtered_dict(vec_dict, vec_words)\n",
    "    return pd.DataFrame.from_dict(vec_dict_filtered, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe\n",
    "# I got this model from here: http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "# Trained on 42 billion words\n",
    "glove_filepath = \"./data/glove_top_200000.txt\"\n",
    "df_glove = get_df_from_txt(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(df_glove.loc[\"I\"])\n",
    "# df_glove.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got word2vec from the google repo and loaded it using gensim\n",
    "# The first header line had to be removed\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format('/Users/andybryant/Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.wv.save_word2vec_format('googlenews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "google_filepath = \"./data/googlenews_top_200000.txt\"\n",
    "df_google = get_df_from_txt(google_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastText\n",
    "# I got this model from here: https://fasttext.cc/docs/en/english-vectors.html\n",
    "# It's the wiki-news 1m vectors dataset\n",
    "ft_filepath = \"./data/fasttext_top_200000.txt\"\n",
    "df_ft = get_df_from_txt(ft_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sets of the different words\n",
    "glove_words_final = set(df_glove.index.tolist())\n",
    "google_words_final = set(df_google.index.tolist())\n",
    "ft_words_final = set(df_ft.index.tolist())\n",
    "# Get their intersection - the words that appear3 in all of them \n",
    "intersection = glove_words_final.intersection(google_words_final, ft_words_final)\n",
    "# Get their union - the words that appear in at least one of them\n",
    "union = glove_words_final.union(google_words_final, ft_words_final)\n",
    "# The ones that do not appear in all three\n",
    "diff = union.symmetric_difference(intersection)\n",
    "print(f'Num words that the sets share: {len(intersection)}')\n",
    "print(f'Num words that the sets do not share: {len(diff)}')\n",
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with words that are not present in every dataframe\n",
    "df_glove.drop(diff, errors='ignore', inplace=True)\n",
    "df_google.drop(diff, errors='ignore', inplace=True)\n",
    "df_ft.drop(diff, errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a multindex dataframe with all of the intersection words in descending order of num appearances\n",
    "data = {'glove' : df_glove, 'word2vec' : df_google, 'fasttext': df_ft}\n",
    "midx = pd.MultiIndex.from_product([list(df_glove.index), data.keys()]) \n",
    "res = pd.concat(data, axis=0, keys=data.keys()).swaplevel(i=0,j=1,axis=0)\n",
    "df_all_vectors = res.sort_index(level=0).reindex(midx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voila! A dataframe with all of the words and their corresponding vectors in descending order or\n",
    "# num occurrences for glove, word2vec, and fasttext.\n",
    "df_all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_vectors.to_csv('./data/all_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_vectors.to_pickle('./data/all_vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glove.to_pickle('./data/glove_vectors.pkl')\n",
    "df_google.to_pickle('./data/google_vectors.pkl')\n",
    "df_ft.to_pickle('./data/fasttext_vectors.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}